---

title: "MLReview_Day3 -Basic concepts"
excerpt: "Dicision Tress and Ensemble methods "

categories:
  - ğŸ“š Machine Learning
tags:
  - [MLReview, PRJ_25]

permalink: /ğŸ“š Machine Learning/MLReview_Day3/

toc: true
toc_sticky: true

date: 2025-03-22
last_modified_at: 2022-03-22

---

## ğŸ¦¥ Dicision Tress and Ensemble Methods

### ***Definition of Dicision Trees and Ensemble Methods***

  - ***Dicision Trees*** : a diagram that shows the possible outcomes of a series of choices.
    a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks.
    Dicision Tress can be applied to both ***classification*** and ***regression problems***, making them a versatile tool in supervised learning.

<img width="897" alt="image" src="https://github.com/user-attachments/assets/6fc22e69-d477-4cc3-81c0-5965d0156083" />

    
  - ***Key Concepts*** :

                    â€¢	ë£¨íŠ¸ ë…¸ë“œ(Root Node): íŠ¸ë¦¬ì˜ ì‹œì‘ì , ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±(feature)ìœ¼ë¡œ ë¶„ê¸° ì‹œì‘
    
                  	â€¢	ë‚´ë¶€ ë…¸ë“œ(Internal Nodes): ì¡°ê±´ì— ë”°ë¼ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì 
    
                  	â€¢	ê°€ì§€(Branches): ì¡°ê±´ ê²°ê³¼ì— ë”°ë¼ ì´ì–´ì§€ëŠ” ê²½ë¡œ
    
                  	â€¢	ë¦¬í”„ ë…¸ë“œ(Leaf Nodes): ìµœì¢…ì ì¸ ì˜ˆì¸¡ ë˜ëŠ” ë¶„ë¥˜ ê²°ê³¼ê°€ ìœ„ì¹˜

                    - ì¥ì :
                    	â€¢	ì§ê´€ì ì´ê³  í•´ì„ì´ ì‰¬ì›€
                    	â€¢	ì „ì²˜ë¦¬ë‚˜ ìŠ¤ì¼€ì¼ë§ ì—†ì´ë„ ì‘ë™ ê°€ëŠ¥
                    	â€¢	ë²”ì£¼í˜•/ì—°ì†í˜• ë³€ìˆ˜ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
                    
                    - ë‹¨ì :
                    	â€¢	ê³¼ì í•©(overfitting) ìœ„í—˜ì´ ìˆìŒ
                    	â€¢	ì‘ì€ ë³€í™”ì—ë„ íŠ¸ë¦¬ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ (ë¶ˆì•ˆì •ì„±)

### ***Ensembe Methods***

- Ensemble methods in machine learning combine multiple models (weak learners) to create a stronger, more accurate predictive model (strong learner) by leveraging the "wisdom of the crowd" principle.

                    â€¢ Bagging :
                                Trains multiple models on different subsets of the training data (created using bootstrap sampling). 
                                Averages the predictions of these models to reduce variance and improve generalization. 
                                Example: Random Forest 
  
                    â€¢ Boosting : 
#### 

###

###

###

###
