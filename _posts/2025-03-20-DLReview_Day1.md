---
title: "DLReview_Day1 : Generative AI for Images "
excerpt: " VAEs and GANs / how diffusion models are trained and sampling is conducted to generate image"

categories:
  - ğŸ“š Deep Learning
tags:
  - [DLReview, Diffusion Model]

permalink: /ğŸ“š Deep Learning/post-name-here/

toc: true
toc_sticky: true

date: 2025-03-20
last_modified_at: 2025-03-20
---

## ğŸ¦¥ DLReview_Day1 : Generative AI for Images 

### 1ï¸âƒ£ Supervised Learning & Unsupervised Lerning 

### 2ï¸âƒ£ 3 deep-learning based generative models

- **Autoencoder** 
- **Variational autoencoder**
- **Generative adversarial networks**

#### Autoencoder 


---

#### VAE 

<img width="521" alt="image" src="https://github.com/user-attachments/assets/c5c926ab-0b59-4558-a53f-3ed687f75604" />
    
    
- At the end of the encoder, you get a highly compressed representation (latent vector z) 
  - This latent vertor z connects to the decoder
  - The z factor is obtained by compressing the input data, often using techniques like Mean Squared Error (MSE) for optimization.

- Once model is trained, we have the latent vector z
  - We can tweak z(which is a long vector) to generate new, similar images.
  - By adjusting the values in z, we can explore variations in the generated ouput 


##### How can we introduce some â€˜randomnessâ€™ into the generation process?

- Instead of learning the latent vectors directly, can we learn the
distribution of each element? = approximate with mean ğœ‡ and standard deviation
- We assume the distributions are Norma



##### How VAE Works 

  1. **Encoder**
     - Take input data (ex.image) and compresses it into a latent representation.
     - Outputs two values: mean (Î¼) and standard deviation (Ïƒ), which define the distribution of the latent variable z
       - í‰ê· (Î¼): ê·¸ë¦¼ì˜ ì£¼ìš” íŠ¹ì§•ì„ ëŒ€í‘œí•˜ëŠ” ê°’ì´ì—ìš”. ì˜ˆë¥¼ ë“¤ì–´, ê°•ì•„ì§€ ê·¸ë¦¼ì´ë¼ë©´ ê·€, ê¼¬ë¦¬, í„¸ ê°™ì€ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ìì˜ˆìš”.

       - í‘œì¤€í¸ì°¨(Ïƒ): ê·¸ë¦¼ì´ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ì—ìš”. ì˜ˆë¥¼ ë“¤ì–´, ê°•ì•„ì§€ ê·¸ë¦¼ì´ë¼ë©´ ê·€ ëª¨ì–‘ì´ ì¡°ê¸ˆì”© ë‹¬ë¼ì§ˆ ìˆ˜ ìˆ                   ëŠ” ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ìš”.

       âœ… latent representation : compressing data and only taking important data 
 
  3. **Sampling**
      - Samples the latent variable z from the distribution defined by Î¼ and Ïƒ
      - This adds randomness, allowing the generation of diverse outputs
 
  4. **Decoder**
       - Take the sampled latent variable z and reconstructs the original data (ex.an image)
       - The goal is make the reconstructed data as close as possible to the original input
  5. **Lossfunction**
       - **Reconstruction Loss** : Measures how well the decoder reconstructs the original data
       - **Regularization Term** : 
       - âœ… reparameterization : it is part of the encoder since it is before Z

**Key Idea** : VAE learns to compress data into a latent space and then reconstruct it, while ensuring the latent space is structured and can generate new, similar data. 

---


#### GANs
- One of the Genertive model that can create realistic fake data. 
- The idea of Gans is concept that one model is **Generator** and another is **Discriminator**
- Main Components of GANs : **Generator** , **Discriminator**
  
##### **Generator** :
  > - Generate fake data from random noise.
  > - Its goal is to generate data that looks real

##### **Discriminator** :
  >- Judge whther the input is real of fake.
  >- Its goal is to accurately distinguish between real and fake data

 ğŸ“Œ They are doing competition,both networks improve over time through this competition.


---


**What is noise?**
- The set of randon generated numbers ex) [0.1,-0.3,0.8,0.2]
- The number are provided generally from normal distribution or uniform distribution. 
- ë…¸ì´ì¦ˆëŠ” generatorì—ê²Œ "ì—¬ê¸°ì„œ ì‹œì‘í•´ì„œ ë¬´ì–¸ê°€ ë§Œë“¤ì–´ë´!"ë¼ê³  ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.

**Why we need noise?**
- Providing startpoint : Generator makes a image based on the information of noise. If there is no noise, generator doesn't know where to start.
- making diversity :generators use diffiernt noise each time, allowing them to create a variety of images. For example, even when drawing the same dog, different noise can result in different appearances of the dog.
- giving creativity : Different noise helps generator create entirely new images. 
  






